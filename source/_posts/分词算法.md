---
title: 分词算法
date: 2021-05-16 14:41:12
summary:  分词算法总结
tags:
  - 分词
  - 算法
categories: 算法
---



转载

# NLP中的标记算法（tokenization）

## 词级标记 (Word level tokenization)

词级标记就是用空格和标点符号讲一段文本分割成许多词语，词级标记标记的最小维度是词语；
虽然词级标记是一种符合常识的标记方法，但是他也存在着诸多问题，这里拿英文词级标记来举例说明

- New York 本是一个词语，但是如果通过常规的词级标记处理方法New York 便会被拆分成两个词，就会丧失其本意，在英文中这样的组合词还有很多并且这种词每时每刻都在产生
- can’t 也会被分割成 can 和 t 这明显也是不符合逻辑的
- 单词 burger 和 birger 只差一个字母，但却有这完全不同的意思， 如果文章的作者将 burger 错写成了 birger ，那么会对NLP任务造成巨大的困难
- 在英文中同一个词一般包含多种事态，如果将所有时态都进行标记，那么不仅会造成资源浪费还回给模型训练增加压力



## 字符级标记 (Character level tokenization)

Karpathy于2015年首次引入该方法 ，字符级标记不是将文本拆分为单词，而是将其拆分为字符，例如：happy 标记为 h a p p y。 词汇量大大减少到该语言中的字符数，英语是字母数26再加上特殊字符。 拼写错误或稀有单词可以更好地处理，因为它们被分解为字符，并且这些字符在词汇表中已经为人所知。
减少词汇量需要权衡序列长度。 现在，每个单词都被分解成所有字符，标记化序列比初始文本长得多。 happy 一词将转换为5个不同的token。 此外没有实现标记化的主要目的，因为至少在英语中字符没有语义。 只有将字符连接在一起才能获得意义。 作为单词和字符标记化之间的中间人，子词标记化产生子词单位 ，该子词单位小于单词，但大于字符。
这里着重说明一下，字符级标记不需要 embedding 而词级标记需要embedding 因为字符级标记等价于已经通过字符UI==对没个词进行编码了



## 子字级标记 (Subword level tokenization)

子词级标记化不会转换最常见的词，并以有意义的子词为单位分解稀有词。 如果将“unfriendly”标记为稀有单词，它将分解为“ un-friend-ly”，它们都是有意义的单位，“ un”的含义相反，“ friend”是名词，“ ly”将其转换为副词。 这里的挑战是如何进行细分，如何使我们将‘unfriendly’分解成 ‘un-friend-ly’ 而不是 ‘unfr-ien-dly’。
截至2020年，基于Transformers的最先进的深度学习架构使用子词级标记。 BERT针对此示例进行以下标记化：

> 原始文本： I have a new GPU.
> 标记文本：[‘i’，‘have’，‘a’，‘new’，‘gp’，’## u’，’。’]

词汇表中存在的单词本身被标记为单词，但是在词汇表中找不到“ GPU”，因此将其视为稀有单词。 根据算法，决定将其分段为“ gp-u”。 “ u”之前的##表示此子字与上一个子字属于同一字。 BPE，Unigram LM，WordPiece和SentencePiece是最常见的子词标记化算法，笔者猜测这几种子词标记化算法应该是在字词的划分策略上有所不同，

## WordPiece 子字级标记算法

而现在大火的bert框架就是使用的WordPiece算法。WordPiece( Schuster和Nakajima，2012年 )最初用于解决日文和韩文语音问题，目前因在BERT中使用而闻名；它在许多方面与BPE相似，不同之处在于它基于似然而不是下一个最高频率对形成一个新的子字。WordPiece底层算法和代码尚未公开，这里简单叙述以下实现步骤。

- 获得足够大的语料库。
- 定义所需的子词词汇量。
- 将单词拆分为字符序列。
- 用文本中的所有字符初始化词汇表。
- 根据词汇建立语言模型。
- 通过将当前词汇表中的两个单元组合以将词汇表增加一个来生成新的子词单元。 从所有可能性中选择新的子词单位，这会在添加到模型时最大程度地增加训练数据的可能性。
- 重复第5步，直到达到子词词汇量(在第2步中定义)，或者似然性增加降至某个阈值以下。

看到这里，你可能不清楚WordPiece是如何选取子词的。这里，通过形式化方法，能够清楚地理解WordPiece在合并这一步是如何作出选择的。假设句子![[公式]](https://www.zhihu.com/equation?tex=+S%3D%28t_%7B1%7D%2Ct_%7B2%7D%2C...%2Ct_%7Bn%7D%29)由n个子词组成，![[公式]](https://www.zhihu.com/equation?tex=t_%7Bi%7D)表示子词，且假设各个子词之间是独立存在的，则句子![[公式]](https://www.zhihu.com/equation?tex=S)的语言模型似然值等价于所有子词概率的乘积：

![[公式]](https://www.zhihu.com/equation?tex=logP%28S%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7DlogP%28t_%7Bi%7D%29+%5C%5C)

假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子![[公式]](https://www.zhihu.com/equation?tex=S)似然值的变化可表示为：

![[公式]](https://www.zhihu.com/equation?tex=logP%28t_%7Bz%7D%29-%28logP%28t_%7Bx%7D%29%2BlogP%28t_%7By%7D%29%29%3Dlog%28%5Cfrac+%7BP%28t_%7Bz%7D%29%7D%7BP%28t_%7Bx%7D%29P%28t_%7By%7D%29%7D%29+%5C%5C)

从上面的公式，很容易发现，似然值的变化就是两个子词之间的互信息。简而言之，WordPiece每次选择合并的两个子词，他们具有最大的互信息值，也就是两子词在语言模型上具有较强的关联性，它们经常在语料中以相邻方式同时出现。



## BPE（Basic Periodontal Examination）

最早是一种数据压缩算法，由Sennrich等人介绍。 在2015年 ，它迭代地合并最频繁出现的字符或字符序列。 该算法大致是这样工作的：

- 获得足够大的语料库。

- 定义所需的子词词汇量。

- 将单词拆分为字符序列，并附加一个特殊的标记，分别显示单词的开头或单词的结尾词缀/后缀。

- 计算文本中的序列对及其频率。 例如，(‘t’，‘h’)具有频率X，(‘h’，‘e’)具有频率Y。

- 根据最频繁出现的序列对生成一个新的子词。 例如，如果(‘t’，‘h’)在该对对中具有最高的频率，则新的子字单元将变为’th’。

- 从第3步开始重复，直到达到子词词汇量(在第2步中定义)或下一个最高频率对为1。在示例中，语料库中的“ t”，“ h”将替换为“ th”，再次计算，最频繁的对再次获得，并再次合并。

  

BPE是一种贪婪的确定性算法，不能提供多个细分。 也就是说，对于给定的文本标记化文本始终是相同的。


下面以一个例子来说明。假设有语料集经过统计后表示为{'low':5,'lower':2,'newest':6,'widest':3}，其中数字代表的是对应单词在语料中的频数。

1) 拆分单词成最小单元，并初始化词表。这里，最小单元为字符，因而，可得到

![img](https://pic4.zhimg.com/80/v2-a2bec0a9fbc6f1c1dea9647a5cb4434f_720w.jpg)

需要注意的是，在将单词拆分成最小单元时，要在单词序列后加上”</w>”(具体实现上可以使用其它符号)来表示中止符。在子词解码时，中止符可以区分单词边界。

2) 在语料上统计相邻单元的频数。这里，最高频连续子词对"e"和"s"出现了6+3=9次，将其合并成"es"，有

![img](https://pic4.zhimg.com/80/v2-fad89fd55ea5db62fcdd0bcadb4e00f7_720w.jpg)

由于语料中不存在's'子词了，因此将其从词表中删除。同时加入新的子词'es'。一增一减，词表大小保持不变。

3) 继续统计相邻子词的频数。此时，最高频连续子词对"es"和"t"出现了6+3=9次, 将其合并成"est"，有

![img](https://pic4.zhimg.com/80/v2-1d66547083a7ff74010d5dbd68b49fa7_720w.jpg)

4) 接着，最高频连续子词对为"est"和"</w>"，有

![img](https://pic3.zhimg.com/80/v2-1c296a454be36a068624e7dbcdb6b63e_720w.jpg)

5) 继续上述迭代直到达到预设的Subword词表大小或下一个最高频的字节对出现频率为1。

从上面的示例可以知道，每次合并后词表大小可能出现3种变化：

- +1，表明加入合并后的新子词，同时原来的2个子词还保留（2个字词分开出现在语料中）。
- +0，表明加入合并后的新子词，同时原来的2个子词中一个保留，一个被消解（一个子词完全随着另一个子词的出现而紧跟着出现）。
- -1，表明加入合并后的新子词，同时原来的2个子词都被消解（2个字词同时连续出现）。

实际上，随着合并的次数增加，词表大小通常先增加后减小。

在得到Subword词表后，针对每一个单词，我们可以采用如下的方式来进行编码：

1. 将词典中的所有子词按照长度由大到小进行排序；
2. 对于单词w，依次遍历排好序的词典。查看当前子词是否是该单词的子字符串，如果是，则输出当前子词，并对剩余单词字符串继续匹配。
3. 如果遍历完字典后，仍然有子字符串没有匹配，则将剩余字符串替换为特殊符号输出，如”<unk>”。
4. 单词的表示即为上述所有输出子词。

解码过程比较简单，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。



## Unigram Language Model (ULM)

与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。

我们接下来看看ULM是如何操作的。

对于句子S，![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+%7Bx%7D%3D%28x_%7B1%7D%2Cx_%7B2%7D%2C...%2Cx_%7Bm%7D%29)为句子的一个分词结果，由m个子词组成。所以，当前分词下句子S的似然值可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=P%28%5Cvec+%7Bx%7D%29+%3D+%5Cprod_%7Bi%3D1%7D%5E%7Bm%7DP%28x_%7Bi%7D%29+%5C%5C)

对于句子S，挑选似然值最大的作为分词结果，则可以表示为

![[公式]](https://www.zhihu.com/equation?tex=x%5E%7B%2A%7D%3Darg%5C%2C%5Cmax_%7Bx+%5Cin+%7BU%28x%29%7D%7D+P%28%5Cvec+%7Bx%7D%29+%5C%5C)

这里![[公式]](https://www.zhihu.com/equation?tex=U%28x%29)包含了句子的所有分词结果。在实际应用中，词表大小有上万个，直接罗列所有可能的分词组合不具有操作性。针对这个问题，可通过维特比算法得到![[公式]](https://www.zhihu.com/equation?tex=x%5E%7B%2A%7D)来解决。

那怎么求解每个子词的概率![[公式]](https://www.zhihu.com/equation?tex=P%28x_%7Bi%7D%29)呢？ULM通过EM算法来估计。假设当前词表V, 则M步最大化的对象是如下似然函数：

![[公式]](https://www.zhihu.com/equation?tex=L+%3D+%5Csum_%7Bs%3D1%7D%5E%7B%7CD%7C%7Dlog%28P%28X%5E%7B%28s%29%7D%29%29%3D%5Csum_%7Bs%3D1%7D%5E%7B%7CD%7C%7Dlog%28%5Csum_%7Bx%5Cin+U%28X%5E%7B%28s%29%7D%29%7D+P%28x%29%29+%5C%5C)

其中，|D|是语料库中语料数量。上述公式的一个直观理解是，将语料库中所有句子的所有分词组合形成的概率相加。

但是，初始时，词表V并不存在。因而，ULM算法采用不断迭代的方法来构造词表以及求解分词概率：

## SentencePiece

如何使用上述子词算法？一种简便的方法是使用SentencePiece，它是谷歌推出的子词开源工具包，其中集成了BPE、ULM子词算法。除此之外，SentencePiece还能支持字符和词级别的分词。更进一步，为了能够处理多语言问题，sentencePiece将句子视为Unicode编码序列，从而子词算法不用依赖于语言的表示。